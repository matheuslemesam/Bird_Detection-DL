{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matheuslemesam/Bird_Detection-DL/blob/main/Bird_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKEGPYsrzRyX"
      },
      "source": [
        "# **Bird Detection, projeto de detecção de espécies de pássaros utilizando Redes Convolucionais. UnB/FCTE - 2025.2 - Professor Vinicius Rispoli**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAdNU7780jm0"
      },
      "source": [
        "**Para começar o projeto, definimos possíveis arquiteturas mais promissoras: entre elas YOLO, U-Net, EfficientNetV2-L, EfficientNet-B4, ConvNeXt-Tiny. A EfficientNetV2-L foi a que mais se destacou pelo fato de ter uma precisão melhor, treinamento mais robusto e uma melhor tecnologia de detecção.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChLX_ho7rqzk"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNWj58NgQmzl"
      },
      "source": [
        "### **Importação do Dataset do Google Drive: montar o Google Drive para acessar o dataset de pássaros.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAj_3vWyElz0"
      },
      "source": [
        "Começamos iniciando a GPU e vendo se foi iniciada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26E90MC6Ek9E"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuvIcl3MQzh8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Montar o Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir caminhos para Google Colab\n",
        "base_path = '/content/drive/MyDrive/Dataset_Aves'\n",
        "dataset_path = os.path.join(base_path, 'original')\n",
        "output_path = os.path.join(base_path, 'augmentation')\n",
        "\n",
        "# Verificar se o dataset existe\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"Dataset encontrado em: {dataset_path}\")\n",
        "\n",
        "    # Listar as espécies disponíveis\n",
        "    species = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "    print(f\"Espécies encontradas ({len(species)}): {species}\")\n",
        "\n",
        "    # Contar imagens por espécie\n",
        "    total_images = 0\n",
        "    for specie in species:\n",
        "        specie_path = os.path.join(dataset_path, specie)\n",
        "        img_count = len([f for f in os.listdir(specie_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"   {specie}: {img_count} imagens\")\n",
        "        total_images += img_count\n",
        "\n",
        "    print(f\"\\nTotal de imagens no dataset: {total_images}\")\n",
        "    print(f\"Pasta de entrada: {dataset_path}\")\n",
        "    print(f\"Pasta de saída: {output_path}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Dataset não encontrado em: {dataset_path}\")\n",
        "    print(\"Verifique o caminho do dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8vGZQPXNPS"
      },
      "source": [
        "## **Data Augmentation: aumentar os dados de forma artificial, neste caso com rotações, translações e espelhamento.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFp5ZAh0Chxw"
      },
      "source": [
        "Para o data augmentation, primeiramente importamos as bibliotecas necessárias, Pytorch e Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdoxcn99Bzs1"
      },
      "outputs": [],
      "source": [
        "!pip install -U torch torchvision torchaudio\n",
        "!pip install -U keras scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMF5UJeSXMPy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def rotate_image_torch(image, angle):\n",
        "    \"\"\"Rotaciona a imagem por um ângulo específico usando PyTorch\"\"\"\n",
        "    return TF.rotate(image, angle, fill=0)\n",
        "\n",
        "def flip_horizontal_torch(image):\n",
        "    \"\"\"Espelha a imagem horizontalmente usando PyTorch\"\"\"\n",
        "    return TF.hflip(image)\n",
        "\n",
        "def translate_image_torch(image, tx, ty):\n",
        "    \"\"\"Translada a imagem usando PyTorch\"\"\"\n",
        "    return TF.affine(image, angle=0, translate=[tx, ty], scale=1, shear=0, fill=0)\n",
        "\n",
        "def load_image_pil(path):\n",
        "    \"\"\"Carrega imagem como PIL Image\"\"\"\n",
        "    return Image.open(path).convert('RGB')\n",
        "\n",
        "def save_image_pil(image, path):\n",
        "    \"\"\"Salva a imagem PIL no caminho especificado\"\"\"\n",
        "    image.save(path, 'JPEG', quality=95)\n",
        "\n",
        "# Definir transformações base\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Redimensionar para tamanho padrão\n",
        "    # Não aplicamos ToTensor aqui pois queremos manter como PIL Image para salvar\n",
        "])\n",
        "\n",
        "print(\"Funções de augmentation PyTorch carregadas com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiAd-5qJqaJN"
      },
      "outputs": [],
      "source": [
        "def augment_bird_dataset_torch(input_dir, output_dir):\n",
        "\n",
        "    # Criar diretório de saída principal\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Ângulos de rotação (de 30 em 30 graus)\n",
        "    angles = [30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]\n",
        "\n",
        "    # Translações (pixels de deslocamento)\n",
        "    translations = [\n",
        "        (50, 0),    # direita\n",
        "        (-50, 0),   # esquerda\n",
        "        (0, 50),    # baixo\n",
        "        (0, -50),   # cima\n",
        "        (35, 35),   # diagonal inferior direita\n",
        "        (-35, 35),  # diagonal inferior esquerda\n",
        "        (35, -35),  # diagonal superior direita\n",
        "        (-35, -35), # diagonal superior esquerda\n",
        "    ]\n",
        "\n",
        "    total_images = 0\n",
        "    total_augmented = 0\n",
        "\n",
        "    # Processar cada espécie (cada pasta dentro de input_dir)\n",
        "    species_dirs = [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]\n",
        "\n",
        "    print(f\"Espécies encontradas para augmentation: {len(species_dirs)}\")\n",
        "    for species in species_dirs:\n",
        "        species_path = os.path.join(input_dir, species)\n",
        "        img_count = len([f for f in os.listdir(species_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"   {species}: {img_count} imagens\")\n",
        "\n",
        "    for species in tqdm(species_dirs, desc=\"Processando espécies\"):\n",
        "        species_input_dir = os.path.join(input_dir, species)\n",
        "        species_output_dir = os.path.join(output_dir, species)  # Mantém a mesma estrutura de pastas\n",
        "        os.makedirs(species_output_dir, exist_ok=True)\n",
        "\n",
        "        # Listar todas as imagens da espécie atual\n",
        "        image_files = [f for f in os.listdir(species_input_dir)\n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        print(f\"\\nProcessando {species}: {len(image_files)} imagens\")\n",
        "\n",
        "        for img_file in tqdm(image_files, desc=f\"Augmentando {species}\", leave=False):\n",
        "            img_path = os.path.join(species_input_dir, img_file)\n",
        "            img_name = os.path.splitext(img_file)[0]\n",
        "\n",
        "            try:\n",
        "                # Carregar imagem como PIL Image\n",
        "                image = load_image_pil(img_path)\n",
        "                # Aplicar resize padrão\n",
        "                image = base_transform(image)\n",
        "\n",
        "                total_images += 1\n",
        "                augmentation_count = 0\n",
        "\n",
        "                # 1. ROTAÇÕES DA IMAGEM ORIGINAL (11 augmentações)\n",
        "                for angle in angles:\n",
        "                    rotated = rotate_image_torch(image, angle)\n",
        "                    output_filename = os.path.join(species_output_dir, f\"{img_name}_rot_{angle}.jpg\")\n",
        "                    save_image_pil(rotated, output_filename)\n",
        "                    augmentation_count += 1\n",
        "\n",
        "                # 2. ESPELHAMENTO HORIZONTAL DA IMAGEM ORIGINAL (1 augmentação)\n",
        "                flipped = flip_horizontal_torch(image)\n",
        "                output_filename = os.path.join(species_output_dir, f\"{img_name}_flip_h.jpg\")\n",
        "                save_image_pil(flipped, output_filename)\n",
        "                augmentation_count += 1\n",
        "\n",
        "                # 3. ROTAÇÕES DA IMAGEM ESPELHADA (11 augmentações)\n",
        "                for angle in angles:\n",
        "                    rotated_flipped = rotate_image_torch(flipped, angle)\n",
        "                    output_filename = os.path.join(species_output_dir, f\"{img_name}_flip_rot_{angle}.jpg\")\n",
        "                    save_image_pil(rotated_flipped, output_filename)\n",
        "                    augmentation_count += 1\n",
        "\n",
        "                # 4. TRANSLAÇÕES DA IMAGEM ORIGINAL (8 augmentações)\n",
        "                for i, (tx, ty) in enumerate(translations):\n",
        "                    translated = translate_image_torch(image, tx, ty)\n",
        "                    output_filename = os.path.join(species_output_dir, f\"{img_name}_trans_{i+1}.jpg\")\n",
        "                    save_image_pil(translated, output_filename)\n",
        "                    augmentation_count += 1\n",
        "\n",
        "                # 5. TRANSLAÇÕES DA IMAGEM ESPELHADA (8 augmentações)\n",
        "                for i, (tx, ty) in enumerate(translations):\n",
        "                    translated_flipped = translate_image_torch(flipped, tx, ty)\n",
        "                    output_filename = os.path.join(species_output_dir, f\"{img_name}_flip_trans_{i+1}.jpg\")\n",
        "                    save_image_pil(translated_flipped, output_filename)\n",
        "                    augmentation_count += 1\n",
        "\n",
        "                total_augmented += augmentation_count\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao processar {img_path}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"\\nData Augmentation concluído\")\n",
        "    print(f\"Imagens originais processadas: {total_images}\")\n",
        "    print(f\"Total de augmentações geradas: {total_augmented}\")\n",
        "    print(f\"Fator de aumento: {total_augmented/total_images:.1f}x por imagem\")\n",
        "    print(f\"Dataset augmentado salvo em: {output_dir}\")\n",
        "    print(f\"\\nEstrutura criada:\")\n",
        "    print(f\"   {output_dir}/\")\n",
        "    for species in [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]:\n",
        "        print(f\"   ├── {species}/\")\n",
        "\n",
        "print(\"Função de augmentation PyTorch preparada para estrutura Dataset_Aves/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UUHl5KYx3qI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm # Para uma bela barra de progresso!\n",
        "\n",
        "\n",
        "source_dir = dataset_path # Onde estão as imagens originais\n",
        "target_dir = output_path # Onde as novas imagens serão salvas\n",
        "target_images_per_class = 1000\n",
        "\n",
        "# Nota: NÃO usamos ToTensor() ou Normalize() aqui, pois queremos salvar\n",
        "# as imagens como arquivos .jpg/.png, não como tensores.\n",
        "augmentation_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(299, scale=(0.7, 1.0)), # Corta e redimensiona de forma mais agressiva\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30), # Aumenta a rotação\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=10), # Pequenas distorções\n",
        "])\n",
        "\n",
        "print(f\"Iniciando o processo de aumento de dados.\")\n",
        "print(f\"Diretório de origem: {source_dir}\")\n",
        "print(f\"Diretório de destino: {target_dir}\\n\")\n",
        "\n",
        "# Garante que o diretório de destino principal exista\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Lista todas as classes (que são os subdiretórios)\n",
        "class_names = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
        "\n",
        "for class_name in class_names:\n",
        "    source_class_path = os.path.join(source_dir, class_name)\n",
        "    target_class_path = os.path.join(target_dir, class_name)\n",
        "\n",
        "    # Cria o subdiretório de destino para a classe\n",
        "    os.makedirs(target_class_path, exist_ok=True)\n",
        "\n",
        "    # Lista todas as imagens originais\n",
        "    original_images = [f for f in os.listdir(source_class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    num_original = len(original_images)\n",
        "\n",
        "    print(f\"Processando classe: '{class_name}'\")\n",
        "    print(f\"  Encontradas {num_original} imagens originais.\")\n",
        "\n",
        "    # --- Passo A: Copiar as imagens originais ---\n",
        "    print(f\"  Copiando originais para o destino...\")\n",
        "    for img_name in tqdm(original_images, desc=f\"Copiando {class_name}\"):\n",
        "        source_img_path = os.path.join(source_class_path, img_name)\n",
        "        target_img_path = os.path.join(target_class_path, img_name)\n",
        "        img = Image.open(source_img_path).convert(\"RGB\")\n",
        "        img.save(target_img_path)\n",
        "\n",
        "    # --- Passo B: Gerar novas imagens ---\n",
        "    num_to_generate = target_images_per_class - num_original\n",
        "    if num_to_generate <= 0:\n",
        "        print(f\"  A classe '{class_name}' já possui {num_original} imagens. Nenhuma imagem nova será gerada.\\n\")\n",
        "        continue\n",
        "\n",
        "    print(f\"  Gerando {num_to_generate} novas imagens via augmentation...\")\n",
        "    for i in tqdm(range(num_to_generate), desc=f\"Gerando {class_name}\"):\n",
        "        # Escolhe uma imagem original aleatória como base\n",
        "        random_image_name = random.choice(original_images)\n",
        "        base_image_path = os.path.join(source_class_path, random_image_name)\n",
        "\n",
        "        # Abre a imagem com a biblioteca PIL\n",
        "        with Image.open(base_image_path).convert(\"RGB\") as img:\n",
        "            # Aplica a transformação de augmentation\n",
        "            augmented_img = augmentation_transform(img)\n",
        "\n",
        "            # Salva a nova imagem com um nome único\n",
        "            new_image_name = f\"aug_{i+1}_{random_image_name}\"\n",
        "            save_path = os.path.join(target_class_path, new_image_name)\n",
        "            augmented_img.save(save_path)\n",
        "    print(f\"  Classe '{class_name}' finalizada.\\n\")\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"PROCESSO DE DATA AUGMENTATION CONCLUÍDO!\")\n",
        "print(f\"O novo dataset está pronto em: {target_dir}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTrEwcHjCRl1"
      },
      "source": [
        "## **Treinamento do modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_Hicqamyagk"
      },
      "source": [
        "Começamos especificando qual framework de Deep Learning o Keras usará, neste caso o PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wl5OdpOtuTaV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K35EBTZtzuP_"
      },
      "source": [
        "Após isto importamos o keras e pytorch para a parte das Redes Neurais e numpy, matplotlib, panda e scikit-learn para visualizações de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-IZcxp-1waqR"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV8d-XGpz1EX"
      },
      "source": [
        "Agora, fazemos um debug das importações e tomada de decisão: utilizar GPU caso tenha, caso contrário, utilizar a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p5AFlvkHEu9i"
      },
      "outputs": [],
      "source": [
        "print(f\"Versão do Keras: {keras.__version__}\") # Printa a versão do Keras\n",
        "\n",
        "print(f\"Keras está usando o backend: {keras.backend.backend()}\") # Printa o framework utilizado pelo Keras\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Verifica se tem GPU para utilizar\n",
        "\n",
        "print(f\"Usando o dispositivo: {device}\") # Mostra a GPU/CPU que será utilizada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXSGoW4IaI5a"
      },
      "source": [
        "Agora, importamos a arquitetura do modelo EfficientNetV2 com a rede neural construida camada por camada e o modificamos para utilizarmos apenas a effnetv2_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUe6OUikaB39"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creates a EfficientNetV2 Model as defined in:\n",
        "Mingxing Tan, Quoc V. Le. (2021).\n",
        "EfficientNetV2: Smaller Models and Faster Training\n",
        "arXiv preprint arXiv:2104.00298.\n",
        "import from https://github.com/d-li14/mobilenetv2.pytorch\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "__all__ = ['effnetv2_l']\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "# SiLU (Swish) activation function\n",
        "if hasattr(nn, 'SiLU'):\n",
        "    SiLU = nn.SiLU\n",
        "else:\n",
        "    # For compatibility with old PyTorch versions\n",
        "    class SiLU(nn.Module):\n",
        "        def forward(self, x):\n",
        "            return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, inp, oup, reduction=4):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
        "                SiLU(),\n",
        "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "def conv_3x3_bn(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
        "        super(MBConv, self).__init__()\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = round(inp * expand_ratio)\n",
        "        self.identity = stride == 1 and inp == oup\n",
        "        if use_se:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                SELayer(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # fused\n",
        "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.identity:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class EffNetV2(nn.Module):\n",
        "    def __init__(self, cfgs, num_classes=1000, width_mult=1.):\n",
        "        super(EffNetV2, self).__init__()\n",
        "        self.cfgs = cfgs\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(24 * width_mult, 8)\n",
        "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
        "        # building inverted residual blocks\n",
        "        block = MBConv\n",
        "        for t, c, n, s, use_se in self.cfgs:\n",
        "            output_channel = _make_divisible(c * width_mult, 8)\n",
        "            for i in range(n):\n",
        "                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t, use_se))\n",
        "                input_channel = output_channel\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        # building last several layers\n",
        "        output_channel = _make_divisible(1792 * width_mult, 8) if width_mult > 1.0 else 1792\n",
        "        self.conv = conv_1x1_bn(input_channel, output_channel)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Linear(output_channel, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.001)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def effnetv2_l(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-L model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  32,  4, 1, 0],\n",
        "        [4,  64,  7, 2, 0],\n",
        "        [4,  96,  7, 2, 0],\n",
        "        [4, 192, 10, 2, 1],\n",
        "        [6, 224, 19, 1, 1],\n",
        "        [6, 384, 25, 2, 1],\n",
        "        [6, 640,  7, 1, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}