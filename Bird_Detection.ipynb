{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matheuslemesam/Bird_Detection-DL/blob/main/Bird_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKEGPYsrzRyX"
      },
      "source": [
        "# **Bird Detection, projeto de detecção de espécies de pássaros utilizando Redes Convolucionais. UnB/FCTE - 2025.2 - Professor Vinicius Rispoli**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAdNU7780jm0"
      },
      "source": [
        "**Para começar o projeto, definimos possíveis arquiteturas mais promissoras: entre elas YOLO, U-Net, EfficientNetV2-L, EfficientNet-B4, ConvNeXt-Tiny. A EfficientNetV2-L foi a que mais se destacou pelo fato de ter uma precisão melhor, treinamento mais robusto e uma melhor tecnologia de detecção.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChLX_ho7rqzk"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNWj58NgQmzl"
      },
      "source": [
        "### **Importação do Dataset do Google Drive: montar o Google Drive para acessar o dataset de pássaros.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAj_3vWyElz0"
      },
      "source": [
        "Começamos iniciando a GPU e vendo se foi iniciada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26E90MC6Ek9E",
        "outputId": "86fe677d-e95b-4935-a605-194a298a4cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Sep 30 20:41:32 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuvIcl3MQzh8",
        "outputId": "ca170344-902c-4417-e755-01d8df619f7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset encontrado em: /content/drive/MyDrive/Dataset_Aves/original\n",
            "Espécies encontradas (14): ['amazona_amazonica', 'ara_macao', 'amazona_aestiva', 'forpus_xanthopterygius', 'primolius_maracana', 'anodorhynchus_hyacinthinus', 'brotogeris_chiriri', 'diopsittaca_nobilis', 'psittacara_leucophthalmus', 'ara_ararauna', 'touit_melanonotus', 'ara_chloropterus', 'orthopsittaca_manilatus', 'eupsittula_aurea']\n",
            "   amazona_amazonica: 189 imagens\n",
            "   ara_macao: 172 imagens\n",
            "   amazona_aestiva: 240 imagens\n",
            "   forpus_xanthopterygius: 215 imagens\n",
            "   primolius_maracana: 195 imagens\n",
            "   anodorhynchus_hyacinthinus: 250 imagens\n",
            "   brotogeris_chiriri: 186 imagens\n",
            "   diopsittaca_nobilis: 217 imagens\n",
            "   psittacara_leucophthalmus: 222 imagens\n",
            "   ara_ararauna: 240 imagens\n",
            "   touit_melanonotus: 150 imagens\n",
            "   ara_chloropterus: 203 imagens\n",
            "   orthopsittaca_manilatus: 184 imagens\n",
            "   eupsittula_aurea: 216 imagens\n",
            "\n",
            "Total de imagens no dataset: 2879\n",
            "Pasta de entrada: /content/drive/MyDrive/Dataset_Aves/original\n",
            "Pasta de saída: /content/drive/MyDrive/Dataset_Aves/augmentation\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Montar o Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir caminhos para Google Colab\n",
        "base_path = '/content/drive/MyDrive/Dataset_Aves'\n",
        "dataset_path = os.path.join(base_path, 'original')\n",
        "output_path = os.path.join(base_path, 'augmentation')\n",
        "\n",
        "# Verificar se o dataset existe\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"Dataset encontrado em: {dataset_path}\")\n",
        "\n",
        "    # Listar as espécies disponíveis\n",
        "    species = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "    print(f\"Espécies encontradas ({len(species)}): {species}\")\n",
        "\n",
        "    # Contar imagens por espécie\n",
        "    total_images = 0\n",
        "    for specie in species:\n",
        "        specie_path = os.path.join(dataset_path, specie)\n",
        "        img_count = len([f for f in os.listdir(specie_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"   {specie}: {img_count} imagens\")\n",
        "        total_images += img_count\n",
        "\n",
        "    print(f\"\\nTotal de imagens no dataset: {total_images}\")\n",
        "    print(f\"Pasta de entrada: {dataset_path}\")\n",
        "    print(f\"Pasta de saída: {output_path}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Dataset não encontrado em: {dataset_path}\")\n",
        "    print(\"Verifique o caminho do dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8vGZQPXNPS"
      },
      "source": [
        "## **Data Augmentation: aumentar os dados de forma artificial, neste caso com rotações, translações e espelhamento.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFp5ZAh0Chxw"
      },
      "source": [
        "Para o data augmentation, primeiramente importamos as bibliotecas necessárias, Pytorch e Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdoxcn99Bzs1",
        "outputId": "2152d613-16d6-4df4-b274-6a5ad4b24f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn, keras\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.10.0\n",
            "    Uninstalling keras-3.10.0:\n",
            "      Successfully uninstalled keras-3.10.0\n",
            "Successfully installed keras-3.11.3 scikit-learn-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch torchvision torchaudio\n",
        "!pip install -U keras scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMF5UJeSXMPy",
        "outputId": "ced32908-59b2-4d1a-8e3d-8b60c48f9f2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Funções de augmentation PyTorch carregadas com sucesso!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def rotate_image_torch(image, angle):\n",
        "    \"\"\"Rotaciona a imagem por um ângulo específico usando PyTorch\"\"\"\n",
        "    return TF.rotate(image, angle, fill=0)\n",
        "\n",
        "def flip_horizontal_torch(image):\n",
        "    \"\"\"Espelha a imagem horizontalmente usando PyTorch\"\"\"\n",
        "    return TF.hflip(image)\n",
        "\n",
        "def translate_image_torch(image, tx, ty):\n",
        "    \"\"\"Translada a imagem usando PyTorch\"\"\"\n",
        "    return TF.affine(image, angle=0, translate=[tx, ty], scale=1, shear=0, fill=0)\n",
        "\n",
        "def load_image_pil(path):\n",
        "    \"\"\"Carrega imagem como PIL Image\"\"\"\n",
        "    return Image.open(path).convert('RGB')\n",
        "\n",
        "def save_image_pil(image, path):\n",
        "    \"\"\"Salva a imagem PIL no caminho especificado\"\"\"\n",
        "    image.save(path, 'JPEG', quality=95)\n",
        "\n",
        "# Definir transformações base\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Redimensionar para tamanho padrão\n",
        "    # Não aplicamos ToTensor aqui pois queremos manter como PIL Image para salvar\n",
        "])\n",
        "\n",
        "print(\"Funções de augmentation PyTorch carregadas com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiAd-5qJqaJN",
        "outputId": "fa370655-6bc4-4bbf-e6bb-15838ac76076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Função de augmentation PyTorch preparada para estrutura Dataset_Aves/\n"
          ]
        }
      ],
      "source": [
        "def augment_bird_dataset_torch(input_dir, output_dir):\n",
        "\n",
        "    # Criar diretório de saída principal\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Ângulos de rotação (de 30 em 30 graus)\n",
        "    angles = [30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]\n",
        "\n",
        "    # Translações (pixels de deslocamento)\n",
        "    translations = [\n",
        "        (50, 0),    # direita\n",
        "        (-50, 0),   # esquerda\n",
        "        (0, 50),    # baixo\n",
        "        (0, -50),   # cima\n",
        "        (35, 35),   # diagonal inferior direita\n",
        "        (-35, 35),  # diagonal inferior esquerda\n",
        "        (35, -35),  # diagonal superior direita\n",
        "        (-35, -35), # diagonal superior esquerda\n",
        "    ]\n",
        "\n",
        "    total_images = 0\n",
        "    total_augmented = 0\n",
        "\n",
        "    # Processar cada espécie (cada pasta dentro de input_dir)\n",
        "    species_dirs = [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]\n",
        "\n",
        "    print(f\"Espécies encontradas para augmentation: {len(species_dirs)}\")\n",
        "    for species in species_dirs:\n",
        "        species_path = os.path.join(input_dir, species)\n",
        "        img_count = len([f for f in os.listdir(species_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"   {species}: {img_count} imagens\")\n",
        "\n",
        "    for species in tqdm(species_dirs, desc=\"Processando espécies\"):\n",
        "        species_input_dir = os.path.join(input_dir, species)\n",
        "        species_output_dir = os.path.join(output_dir, species)  # Mantém a mesma estrutura de pastas\n",
        "        os.makedirs(species_output_dir, exist_ok=True)\n",
        "\n",
        "        # Listar todas as imagens da espécie atual\n",
        "        image_files = [f for f in os.listdir(species_input_dir)\n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        print(f\"\\nProcessando {species}: {len(image_files)} imagens\")\n",
        "\n",
        "        for img_file in tqdm(image_files, desc=f\"Augmentando {species}\", leave=False):\n",
        "            img_path = os.path.join(species_input_dir, img_file)\n",
        "            img_name = os.path.splitext(img_file)[0]\n",
        "\n",
        "            try:\n",
        "                # Carregar imagem como PIL Image\n",
        "                image = load_image_pil(img_path)\n",
        "                # Aplicar resize padrão\n",
        "                image = base_transform(image)\n",
        "\n",
        "                total_images += 1\n",
        "                augmentation_count = 0\n",
        "\n",
        "                # 1. ROTAÇÕES DA IMAGEM ORIGINAL (11 augmentações)\n",
        "                for angle in angles:\n",
        "                    rotated = rotate_image_torch(image, angle)\n",
        "                    output_filename = os.path.join(species_output_dir, f\"{img_name}_rot_{angle}.jpg\")\n",
        "                    save_image_pil(rotated, output_filename)\n",
        "                    augmentation_count += 1\n",
        "\n",
        "                # 2. ESPELHAMENTO HORIZONTAL DA IMAGEM ORIGINAL (1 augmentação)\n",
        "                flipped = flip_horizontal_torch(image)\n",
        "                output_filename = os.path.join(species_output_dir, f\"{img_name}_flip_h.jpg\")\n",
        "                save_image_pil(flipped, output_filename)\n",
        "                augmentation_count += 1\n",
        "\n",
        "                # 3. ROTAÇÕES DA IMAGEM ESPELHADA (11 augmentações)\n",
        "                for angle in angles:\n",
        "                    rotated_flipped = rotate_image_torch(flipped, angle)\n",
        "                    output_filename = os.path.join(species_output_dir, f\"{img_name}_flip_rot_{angle}.jpg\")\n",
        "                    save_image_pil(rotated_flipped, output_filename)\n",
        "                    augmentation_count += 1\n",
        "\n",
        "                # 4. TRANSLAÇÕES DA IMAGEM ORIGINAL (8 augmentações)\n",
        "                for i, (tx, ty) in enumerate(translations):\n",
        "                    translated = translate_image_torch(image, tx, ty)\n",
        "                    output_filename = os.path.join(species_output_dir, f\"{img_name}_trans_{i+1}.jpg\")\n",
        "                    save_image_pil(translated, output_filename)\n",
        "                    augmentation_count += 1\n",
        "\n",
        "                # 5. TRANSLAÇÕES DA IMAGEM ESPELHADA (8 augmentações)\n",
        "                for i, (tx, ty) in enumerate(translations):\n",
        "                    translated_flipped = translate_image_torch(flipped, tx, ty)\n",
        "                    output_filename = os.path.join(species_output_dir, f\"{img_name}_flip_trans_{i+1}.jpg\")\n",
        "                    save_image_pil(translated_flipped, output_filename)\n",
        "                    augmentation_count += 1\n",
        "\n",
        "                total_augmented += augmentation_count\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao processar {img_path}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"\\nData Augmentation concluído\")\n",
        "    print(f\"Imagens originais processadas: {total_images}\")\n",
        "    print(f\"Total de augmentações geradas: {total_augmented}\")\n",
        "    print(f\"Fator de aumento: {total_augmented/total_images:.1f}x por imagem\")\n",
        "    print(f\"Dataset augmentado salvo em: {output_dir}\")\n",
        "    print(f\"\\nEstrutura criada:\")\n",
        "    print(f\"   {output_dir}/\")\n",
        "    for species in [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]:\n",
        "        print(f\"   ├── {species}/\")\n",
        "\n",
        "print(\"Função de augmentation PyTorch preparada para estrutura Dataset_Aves/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UUHl5KYx3qI",
        "outputId": "1fc4e27f-045a-4eb9-cd88-0e0074a5f8d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Espaço em disco:\n",
            "Total: 112.0 GB\n",
            "Usado: 39.0 GB\n",
            "Livre: 73.0 GB\n",
            "Espaço suficiente para data augmentation local.\n",
            "\n",
            "USANDO PROCESSAMENTO LOCAL (POR ESPÉCIE) + CÓPIA FINAL PARA DRIVE\n",
            "   Isso resolve problemas de sincronização com Google Drive\n",
            "   Velocidade ~10x mais rápida\n",
            "   Maior resiliência a quedas de conexão do Drive\n",
            "INICIANDO FLUXO DE AUMENTO DE DADOS ROBUSTO (POR ESPÉCIE)\n",
            "Origem (dataset original): /content/drive/MyDrive/Dataset_Aves/original\n",
            "Destino final (dataset aumentado no Drive): /content/drive/MyDrive/Dataset_Aves/augmentation\n",
            "------------------------------------------------------------\n",
            "Preparando destino principal no Google Drive...\n",
            "   Removendo dataset antigo: /content/drive/MyDrive/Dataset_Aves/augmentation\n",
            "   Dataset antigo removido.\n",
            "   Diretório de destino principal criado: /content/drive/MyDrive/Dataset_Aves/augmentation\n",
            "------------------------------------------------------------\n",
            "Total de espécies a serem processadas: 14\n",
            "Espécies: ['amazona_amazonica', 'ara_macao', 'amazona_aestiva', 'forpus_xanthopterygius', 'primolius_maracana', 'anodorhynchus_hyacinthinus', 'brotogeris_chiriri', 'diopsittaca_nobilis', 'psittacara_leucophthalmus', 'ara_ararauna', 'touit_melanonotus', 'ara_chloropterus', 'orthopsittaca_manilatus', 'eupsittula_aurea']\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: amazona_amazonica (1/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/amazona_amazonica\n",
            "   Processamento local temporário: /content/augmentation_temp/amazona_amazonica\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/amazona_amazonica\n",
            "------------------------------\n",
            "Limpando ambiente local para amazona_amazonica...\n",
            "   Diretório temporário criado: /content/augmentation_temp/amazona_amazonica\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 73.1 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para amazona_amazonica...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/amazona_amazonica (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/amazona_amazonica (Local - SSD rápido)\n",
            "\n",
            "Aumentando amazona_amazonica: 189 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para amazona_amazonica concluído.\n",
            "   Imagens originais: 189, Aumentadas: 7371\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para amazona_amazonica...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE amazona_amazonica PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/amazona_amazonica\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/amazona_amazonica\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE amazona_amazonica PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de amazona_amazonica no Drive...\n",
            "   amazona_amazonica no Drive: 7371 imagens\n",
            "   INTEGRIDADE VERIFICADA para amazona_amazonica: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para amazona_amazonica...\n",
            "   Arquivos temporários de amazona_amazonica removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: ara_macao (2/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/ara_macao\n",
            "   Processamento local temporário: /content/augmentation_temp/ara_macao\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/ara_macao\n",
            "------------------------------\n",
            "Limpando ambiente local para ara_macao...\n",
            "   Diretório temporário criado: /content/augmentation_temp/ara_macao\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 72.9 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para ara_macao...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/ara_macao (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/ara_macao (Local - SSD rápido)\n",
            "\n",
            "Aumentando ara_macao: 172 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para ara_macao concluído.\n",
            "   Imagens originais: 172, Aumentadas: 6708\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para ara_macao...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE ara_macao PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/ara_macao\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/ara_macao\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE ara_macao PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de ara_macao no Drive...\n",
            "   ara_macao no Drive: 6708 imagens\n",
            "   INTEGRIDADE VERIFICADA para ara_macao: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para ara_macao...\n",
            "   Arquivos temporários de ara_macao removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: amazona_aestiva (3/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/amazona_aestiva\n",
            "   Processamento local temporário: /content/augmentation_temp/amazona_aestiva\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/amazona_aestiva\n",
            "------------------------------\n",
            "Limpando ambiente local para amazona_aestiva...\n",
            "   Diretório temporário criado: /content/augmentation_temp/amazona_aestiva\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 72.7 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para amazona_aestiva...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/amazona_aestiva (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/amazona_aestiva (Local - SSD rápido)\n",
            "\n",
            "Aumentando amazona_aestiva: 240 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para amazona_aestiva concluído.\n",
            "   Imagens originais: 240, Aumentadas: 9360\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para amazona_aestiva...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE amazona_aestiva PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/amazona_aestiva\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/amazona_aestiva\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE amazona_aestiva PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de amazona_aestiva no Drive...\n",
            "   amazona_aestiva no Drive: 9360 imagens\n",
            "   INTEGRIDADE VERIFICADA para amazona_aestiva: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para amazona_aestiva...\n",
            "   Arquivos temporários de amazona_aestiva removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: forpus_xanthopterygius (4/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/forpus_xanthopterygius\n",
            "   Processamento local temporário: /content/augmentation_temp/forpus_xanthopterygius\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/forpus_xanthopterygius\n",
            "------------------------------\n",
            "Limpando ambiente local para forpus_xanthopterygius...\n",
            "   Diretório temporário criado: /content/augmentation_temp/forpus_xanthopterygius\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 72.5 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para forpus_xanthopterygius...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/forpus_xanthopterygius (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/forpus_xanthopterygius (Local - SSD rápido)\n",
            "\n",
            "Aumentando forpus_xanthopterygius: 215 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para forpus_xanthopterygius concluído.\n",
            "   Imagens originais: 215, Aumentadas: 8385\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para forpus_xanthopterygius...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE forpus_xanthopterygius PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/forpus_xanthopterygius\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/forpus_xanthopterygius\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE forpus_xanthopterygius PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de forpus_xanthopterygius no Drive...\n",
            "   forpus_xanthopterygius no Drive: 8385 imagens\n",
            "   INTEGRIDADE VERIFICADA para forpus_xanthopterygius: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para forpus_xanthopterygius...\n",
            "   Arquivos temporários de forpus_xanthopterygius removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: primolius_maracana (5/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/primolius_maracana\n",
            "   Processamento local temporário: /content/augmentation_temp/primolius_maracana\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/primolius_maracana\n",
            "------------------------------\n",
            "Limpando ambiente local para primolius_maracana...\n",
            "   Diretório temporário criado: /content/augmentation_temp/primolius_maracana\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 72.2 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para primolius_maracana...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/primolius_maracana (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/primolius_maracana (Local - SSD rápido)\n",
            "\n",
            "Aumentando primolius_maracana: 195 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para primolius_maracana concluído.\n",
            "   Imagens originais: 195, Aumentadas: 7605\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para primolius_maracana...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE primolius_maracana PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/primolius_maracana\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/primolius_maracana\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE primolius_maracana PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de primolius_maracana no Drive...\n",
            "   primolius_maracana no Drive: 7605 imagens\n",
            "   INTEGRIDADE VERIFICADA para primolius_maracana: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para primolius_maracana...\n",
            "   Arquivos temporários de primolius_maracana removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: anodorhynchus_hyacinthinus (6/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/anodorhynchus_hyacinthinus\n",
            "   Processamento local temporário: /content/augmentation_temp/anodorhynchus_hyacinthinus\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/anodorhynchus_hyacinthinus\n",
            "------------------------------\n",
            "Limpando ambiente local para anodorhynchus_hyacinthinus...\n",
            "   Diretório temporário criado: /content/augmentation_temp/anodorhynchus_hyacinthinus\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 72.0 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para anodorhynchus_hyacinthinus...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/anodorhynchus_hyacinthinus (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/anodorhynchus_hyacinthinus (Local - SSD rápido)\n",
            "\n",
            "Aumentando anodorhynchus_hyacinthinus: 250 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para anodorhynchus_hyacinthinus concluído.\n",
            "   Imagens originais: 250, Aumentadas: 9750\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para anodorhynchus_hyacinthinus...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE anodorhynchus_hyacinthinus PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/anodorhynchus_hyacinthinus\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/anodorhynchus_hyacinthinus\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE anodorhynchus_hyacinthinus PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de anodorhynchus_hyacinthinus no Drive...\n",
            "   anodorhynchus_hyacinthinus no Drive: 9750 imagens\n",
            "   INTEGRIDADE VERIFICADA para anodorhynchus_hyacinthinus: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para anodorhynchus_hyacinthinus...\n",
            "   Arquivos temporários de anodorhynchus_hyacinthinus removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: brotogeris_chiriri (7/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/brotogeris_chiriri\n",
            "   Processamento local temporário: /content/augmentation_temp/brotogeris_chiriri\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/brotogeris_chiriri\n",
            "------------------------------\n",
            "Limpando ambiente local para brotogeris_chiriri...\n",
            "   Diretório temporário criado: /content/augmentation_temp/brotogeris_chiriri\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 71.8 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para brotogeris_chiriri...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/brotogeris_chiriri (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/brotogeris_chiriri (Local - SSD rápido)\n",
            "\n",
            "Aumentando brotogeris_chiriri: 186 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para brotogeris_chiriri concluído.\n",
            "   Imagens originais: 186, Aumentadas: 7254\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para brotogeris_chiriri...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE brotogeris_chiriri PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/brotogeris_chiriri\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/brotogeris_chiriri\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE brotogeris_chiriri PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de brotogeris_chiriri no Drive...\n",
            "   brotogeris_chiriri no Drive: 7254 imagens\n",
            "   INTEGRIDADE VERIFICADA para brotogeris_chiriri: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para brotogeris_chiriri...\n",
            "   Arquivos temporários de brotogeris_chiriri removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: diopsittaca_nobilis (8/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/diopsittaca_nobilis\n",
            "   Processamento local temporário: /content/augmentation_temp/diopsittaca_nobilis\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/diopsittaca_nobilis\n",
            "------------------------------\n",
            "Limpando ambiente local para diopsittaca_nobilis...\n",
            "   Diretório temporário criado: /content/augmentation_temp/diopsittaca_nobilis\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 71.6 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para diopsittaca_nobilis...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/diopsittaca_nobilis (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/diopsittaca_nobilis (Local - SSD rápido)\n",
            "\n",
            "Aumentando diopsittaca_nobilis: 217 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para diopsittaca_nobilis concluído.\n",
            "   Imagens originais: 217, Aumentadas: 8463\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para diopsittaca_nobilis...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE diopsittaca_nobilis PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/diopsittaca_nobilis\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/diopsittaca_nobilis\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE diopsittaca_nobilis PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de diopsittaca_nobilis no Drive...\n",
            "   diopsittaca_nobilis no Drive: 8463 imagens\n",
            "   INTEGRIDADE VERIFICADA para diopsittaca_nobilis: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para diopsittaca_nobilis...\n",
            "   Arquivos temporários de diopsittaca_nobilis removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: psittacara_leucophthalmus (9/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/psittacara_leucophthalmus\n",
            "   Processamento local temporário: /content/augmentation_temp/psittacara_leucophthalmus\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/psittacara_leucophthalmus\n",
            "------------------------------\n",
            "Limpando ambiente local para psittacara_leucophthalmus...\n",
            "   Diretório temporário criado: /content/augmentation_temp/psittacara_leucophthalmus\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 71.3 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para psittacara_leucophthalmus...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/psittacara_leucophthalmus (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/psittacara_leucophthalmus (Local - SSD rápido)\n",
            "\n",
            "Aumentando psittacara_leucophthalmus: 222 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para psittacara_leucophthalmus concluído.\n",
            "   Imagens originais: 222, Aumentadas: 8658\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para psittacara_leucophthalmus...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE psittacara_leucophthalmus PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/psittacara_leucophthalmus\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/psittacara_leucophthalmus\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE psittacara_leucophthalmus PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de psittacara_leucophthalmus no Drive...\n",
            "   psittacara_leucophthalmus no Drive: 8658 imagens\n",
            "   INTEGRIDADE VERIFICADA para psittacara_leucophthalmus: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para psittacara_leucophthalmus...\n",
            "   Arquivos temporários de psittacara_leucophthalmus removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: ara_ararauna (10/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/ara_ararauna\n",
            "   Processamento local temporário: /content/augmentation_temp/ara_ararauna\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/ara_ararauna\n",
            "------------------------------\n",
            "Limpando ambiente local para ara_ararauna...\n",
            "   Diretório temporário criado: /content/augmentation_temp/ara_ararauna\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 71.1 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para ara_ararauna...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/ara_ararauna (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/ara_ararauna (Local - SSD rápido)\n",
            "\n",
            "Aumentando ara_ararauna: 240 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para ara_ararauna concluído.\n",
            "   Imagens originais: 240, Aumentadas: 9360\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para ara_ararauna...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE ara_ararauna PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/ara_ararauna\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/ara_ararauna\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE ara_ararauna PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de ara_ararauna no Drive...\n",
            "   ara_ararauna no Drive: 9360 imagens\n",
            "   INTEGRIDADE VERIFICADA para ara_ararauna: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para ara_ararauna...\n",
            "   Arquivos temporários de ara_ararauna removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: touit_melanonotus (11/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/touit_melanonotus\n",
            "   Processamento local temporário: /content/augmentation_temp/touit_melanonotus\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/touit_melanonotus\n",
            "------------------------------\n",
            "Limpando ambiente local para touit_melanonotus...\n",
            "   Diretório temporário criado: /content/augmentation_temp/touit_melanonotus\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 70.9 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para touit_melanonotus...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/touit_melanonotus (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/touit_melanonotus (Local - SSD rápido)\n",
            "\n",
            "Aumentando touit_melanonotus: 150 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para touit_melanonotus concluído.\n",
            "   Imagens originais: 150, Aumentadas: 5850\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para touit_melanonotus...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE touit_melanonotus PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/touit_melanonotus\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/touit_melanonotus\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE touit_melanonotus PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de touit_melanonotus no Drive...\n",
            "   touit_melanonotus no Drive: 5850 imagens\n",
            "   INTEGRIDADE VERIFICADA para touit_melanonotus: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para touit_melanonotus...\n",
            "   Arquivos temporários de touit_melanonotus removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: ara_chloropterus (12/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/ara_chloropterus\n",
            "   Processamento local temporário: /content/augmentation_temp/ara_chloropterus\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/ara_chloropterus\n",
            "------------------------------\n",
            "Limpando ambiente local para ara_chloropterus...\n",
            "   Diretório temporário criado: /content/augmentation_temp/ara_chloropterus\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 70.7 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para ara_chloropterus...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/ara_chloropterus (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/ara_chloropterus (Local - SSD rápido)\n",
            "\n",
            "Aumentando ara_chloropterus: 203 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para ara_chloropterus concluído.\n",
            "   Imagens originais: 203, Aumentadas: 7917\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para ara_chloropterus...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE ara_chloropterus PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/ara_chloropterus\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/ara_chloropterus\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE ara_chloropterus PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de ara_chloropterus no Drive...\n",
            "   ara_chloropterus no Drive: 7917 imagens\n",
            "   INTEGRIDADE VERIFICADA para ara_chloropterus: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para ara_chloropterus...\n",
            "   Arquivos temporários de ara_chloropterus removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: orthopsittaca_manilatus (13/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/orthopsittaca_manilatus\n",
            "   Processamento local temporário: /content/augmentation_temp/orthopsittaca_manilatus\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/orthopsittaca_manilatus\n",
            "------------------------------\n",
            "Limpando ambiente local para orthopsittaca_manilatus...\n",
            "   Diretório temporário criado: /content/augmentation_temp/orthopsittaca_manilatus\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 70.5 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para orthopsittaca_manilatus...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/orthopsittaca_manilatus (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/orthopsittaca_manilatus (Local - SSD rápido)\n",
            "\n",
            "Aumentando orthopsittaca_manilatus: 184 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para orthopsittaca_manilatus concluído.\n",
            "   Imagens originais: 184, Aumentadas: 7176\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para orthopsittaca_manilatus...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE orthopsittaca_manilatus PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/orthopsittaca_manilatus\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/orthopsittaca_manilatus\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE orthopsittaca_manilatus PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de orthopsittaca_manilatus no Drive...\n",
            "   orthopsittaca_manilatus no Drive: 7176 imagens\n",
            "   INTEGRIDADE VERIFICADA para orthopsittaca_manilatus: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para orthopsittaca_manilatus...\n",
            "   Arquivos temporários de orthopsittaca_manilatus removidos.\n",
            "------------------------------\n",
            "\n",
            "--- PROCESSANDO ESPÉCIE: eupsittula_aurea (14/14) ---\n",
            "   Origem da espécie: /content/drive/MyDrive/Dataset_Aves/original/eupsittula_aurea\n",
            "   Processamento local temporário: /content/augmentation_temp/eupsittula_aurea\n",
            "   Destino da espécie no Drive: /content/drive/MyDrive/Dataset_Aves/augmentation/eupsittula_aurea\n",
            "------------------------------\n",
            "Limpando ambiente local para eupsittula_aurea...\n",
            "   Diretório temporário criado: /content/augmentation_temp/eupsittula_aurea\n",
            "\n",
            "Verificando espaço em disco local...\n",
            "   Espaço livre no /content: 70.3 GB\n",
            "   Espaço suficiente para processamento local.\n",
            "\n",
            "INICIANDO AUMENTO DE DADOS LOCAL para eupsittula_aurea...\n",
            "   Lendo de: /content/drive/MyDrive/Dataset_Aves/original/eupsittula_aurea (Google Drive)\n",
            "   Escrevendo em: /content/augmentation_temp/eupsittula_aurea (Local - SSD rápido)\n",
            "\n",
            "Aumentando eupsittula_aurea: 216 imagens originais\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Aumento de dados local para eupsittula_aurea concluído.\n",
            "   Imagens originais: 216, Aumentadas: 8424\n",
            "   Fator de aumento: 39.0x\n",
            "------------------------------\n",
            "\n",
            "Verificando conexão com Google Drive para eupsittula_aurea...\n",
            "   Google Drive está conectado.\n",
            "------------------------------\n",
            "\n",
            "COPIANDO DADOS AUMENTADOS DE eupsittula_aurea PARA GOOGLE DRIVE...\n",
            "   De: /content/augmentation_temp/eupsittula_aurea\n",
            "   Para: /content/drive/MyDrive/Dataset_Aves/augmentation/eupsittula_aurea\n",
            "   Esta operação pode demorar um pouco...\n",
            "   CÓPIA DE eupsittula_aurea PARA GOOGLE DRIVE CONCLUÍDA!\n",
            "------------------------------\n",
            "\n",
            "Verificando resultado final de eupsittula_aurea no Drive...\n",
            "   eupsittula_aurea no Drive: 8424 imagens\n",
            "   INTEGRIDADE VERIFICADA para eupsittula_aurea: Todos os arquivos foram copiados corretamente!\n",
            "------------------------------\n",
            "\n",
            "Limpando arquivos temporários locais para eupsittula_aurea...\n",
            "   Arquivos temporários de eupsittula_aurea removidos.\n",
            "------------------------------\n",
            "\n",
            "============================================================\n",
            "FLUXO GERAL CONCLUÍDO!\n",
            "Dataset aumentado disponível em: /content/drive/MyDrive/Dataset_Aves/augmentation\n",
            "\n",
            "Resumo final no Drive (/content/drive/MyDrive/Dataset_Aves/augmentation):\n",
            "   ├── amazona_amazonica: 7371 imagens\n",
            "   ├── ara_macao: 6708 imagens\n",
            "   ├── amazona_aestiva: 9360 imagens\n",
            "   ├── forpus_xanthopterygius: 8385 imagens\n",
            "   ├── primolius_maracana: 7605 imagens\n",
            "   ├── anodorhynchus_hyacinthinus: 9750 imagens\n",
            "   ├── brotogeris_chiriri: 7254 imagens\n",
            "   ├── diopsittaca_nobilis: 8463 imagens\n",
            "   ├── psittacara_leucophthalmus: 8658 imagens\n",
            "   ├── ara_ararauna: 9360 imagens\n",
            "   ├── touit_melanonotus: 5850 imagens\n",
            "   ├── ara_chloropterus: 7917 imagens\n",
            "   ├── orthopsittaca_manilatus: 7176 imagens\n",
            "   ├── eupsittula_aurea: 8424 imagens\n",
            "\n",
            "Total final de imagens no Drive: 112281\n",
            "Total de espécies no Drive: 14\n",
            "   VERIFICADO: Número de espécies no Drive corresponde ao original.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import time # Importar módulo time para adicionar delays\n",
        "\n",
        "def fluxo_aumento_dados_robusto(caminho_dataset, caminho_saida):\n",
        "    \"\"\"\n",
        "    Executa aumento de dados de forma robusta, processando por espécie,\n",
        "    lidando com reconexão do Google Drive e salvando no Drive por espécie.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"INICIANDO FLUXO DE AUMENTO DE DADOS ROBUSTO (POR ESPÉCIE)\")\n",
        "    print(f\"Origem (dataset original): {caminho_dataset}\")\n",
        "    print(f\"Destino final (dataset aumentado no Drive): {caminho_saida}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Limpar o diretório de saída principal no Drive antes de começar, se existir\n",
        "    print(f\"Preparando destino principal no Google Drive...\")\n",
        "    if os.path.exists(caminho_saida):\n",
        "        print(f\"   Removendo dataset antigo: {caminho_saida}\")\n",
        "        shutil.rmtree(caminho_saida)\n",
        "        print(\"   Dataset antigo removido.\")\n",
        "    os.makedirs(caminho_saida, exist_ok=True)\n",
        "    print(f\"   Diretório de destino principal criado: {caminho_saida}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Obter a lista de espécies no diretório original\n",
        "    diretorios_especies = [d for d in os.listdir(caminho_dataset) if os.path.isdir(os.path.join(caminho_dataset, d))]\n",
        "    print(f\"Total de espécies a serem processadas: {len(diretorios_especies)}\")\n",
        "    print(f\"Espécies: {diretorios_especies}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, especie in enumerate(diretorios_especies):\n",
        "        diretorio_entrada_especie = os.path.join(caminho_dataset, especie)\n",
        "        diretorio_saida_especie_drive = os.path.join(caminho_saida, especie) # Diretório de saída para a espécie no Drive\n",
        "\n",
        "        # 1. DEFINIR CAMINHO TEMPORÁRIO LOCAL PARA ESTA ESPÉCIE\n",
        "        caminho_temporario_especie = f'/content/augmentation_temp/{especie}' # Caminho temporário por espécie\n",
        "\n",
        "        print(f\"\\n--- PROCESSANDO ESPÉCIE: {especie} ({i+1}/{len(diretorios_especies)}) ---\")\n",
        "        print(f\"   Origem da espécie: {diretorio_entrada_especie}\")\n",
        "        print(f\"   Processamento local temporário: {caminho_temporario_especie}\")\n",
        "        print(f\"   Destino da espécie no Drive: {diretorio_saida_especie_drive}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "\n",
        "        try:\n",
        "            # 2. LIMPAR AMBIENTE LOCAL PARA ESTA ESPÉCIE\n",
        "            print(f\"Limpando ambiente local para {especie}...\")\n",
        "            if os.path.exists(caminho_temporario_especie):\n",
        "                shutil.rmtree(caminho_temporario_especie)\n",
        "                print(f\"   Diretório temporário removido: {caminho_temporario_especie}\")\n",
        "\n",
        "            os.makedirs(caminho_temporario_especie, exist_ok=True)\n",
        "            print(f\"   Diretório temporário criado: {caminho_temporario_especie}\")\n",
        "\n",
        "            # 3. VERIFICAR ESPAÇO EM DISCO LOCAL (Geral, antes de cada espécie)\n",
        "            print(\"\\nVerificando espaço em disco local...\")\n",
        "            total, usado, livre = shutil.disk_usage('/content')\n",
        "            livre_gb = livre / (1024**3)\n",
        "            print(f\"   Espaço livre no /content: {livre_gb:.1f} GB\")\n",
        "\n",
        "            if livre_gb < 5.0:  # Menos que 5GB por segurança\n",
        "                 print(\"   AVISO: Pouco espaço livre. O processo pode falhar.\")\n",
        "                 # Adicionar uma pausa ou um prompt para o usuário aqui pode ser útil\n",
        "                 # input(\"Pressione Enter para continuar ou Crtl+C para cancelar...\") # Exemplo de pausa\n",
        "\n",
        "            else:\n",
        "                 print(\"   Espaço suficiente para processamento local.\")\n",
        "\n",
        "\n",
        "            # 4. EXECUTAR PROCESSAMENTO LOCAL PARA ESTA ESPÉCIE\n",
        "            print(f\"\\nINICIANDO AUMENTO DE DADOS LOCAL para {especie}...\")\n",
        "            print(f\"   Lendo de: {diretorio_entrada_especie} (Google Drive)\")\n",
        "            print(f\"   Escrevendo em: {caminho_temporario_especie} (Local - SSD rápido)\")\n",
        "\n",
        "            # Lógica de augmentation (copiada/adaptada da augment_bird_dataset_torch)\n",
        "            angulos = [30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]\n",
        "            translacoes = [\n",
        "                (50, 0), (-50, 0), (0, 50), (0, -50),\n",
        "                (35, 35), (-35, 35), (35, -35), (-35, -35),\n",
        "            ]\n",
        "            transformacao_base = transforms.Compose([transforms.Resize((224, 224))]) # Redimensionar\n",
        "\n",
        "            os.makedirs(caminho_temporario_especie, exist_ok=True) # Garantir que o dir de saída da espécie existe localmente\n",
        "\n",
        "            arquivos_imagem = [f for f in os.listdir(diretorio_entrada_especie)\n",
        "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "            total_imagens_especie = 0\n",
        "            total_aumentadas_especie = 0\n",
        "\n",
        "            print(f\"\\nAumentando {especie}: {len(arquivos_imagem)} imagens originais\")\n",
        "\n",
        "            for arquivo_img in tqdm(arquivos_imagem, desc=f\"Aumentando {especie}\", leave=False):\n",
        "                caminho_img = os.path.join(diretorio_entrada_especie, arquivo_img)\n",
        "                nome_img = os.path.splitext(arquivo_img)[0]\n",
        "\n",
        "                try:\n",
        "                    imagem = load_image_pil(caminho_img) # Carregar imagem como PIL Image\n",
        "                    imagem = transformacao_base(imagem)    # Aplicar resize\n",
        "\n",
        "                    total_imagens_especie += 1\n",
        "                    contagem_aumento = 0\n",
        "\n",
        "                    # ROTAÇÕES DA IMAGEM ORIGINAL\n",
        "                    for angulo in angulos:\n",
        "                        rotacionada = rotate_image_torch(imagem, angulo)\n",
        "                        nome_arquivo_saida = os.path.join(caminho_temporario_especie, f\"{nome_img}_rot_{angulo}.jpg\")\n",
        "                        save_image_pil(rotacionada, nome_arquivo_saida)\n",
        "                        contagem_aumento += 1\n",
        "\n",
        "                    # ESPELHAMENTO HORIZONTAL DA IMAGEM ORIGINAL\n",
        "                    espelhada = flip_horizontal_torch(imagem)\n",
        "                    nome_arquivo_saida = os.path.join(caminho_temporario_especie, f\"{nome_img}_flip_h.jpg\")\n",
        "                    save_image_pil(espelhada, nome_arquivo_saida)\n",
        "                    contagem_aumento += 1\n",
        "\n",
        "                    # ROTAÇÕES DA IMAGEM ESPELHADA\n",
        "                    for angulo in angulos:\n",
        "                        rotacionada_espelhada = rotate_image_torch(espelhada, angulo)\n",
        "                        nome_arquivo_saida = os.path.join(caminho_temporario_especie, f\"{nome_img}_flip_rot_{angulo}.jpg\")\n",
        "                        save_image_pil(rotacionada_espelhada, nome_arquivo_saida)\n",
        "                        contagem_aumento += 1\n",
        "\n",
        "                    # TRANSLAÇÕES DA IMAGEM ORIGINAL\n",
        "                    for k, (tx, ty) in enumerate(translacoes):\n",
        "                        transladada = translate_image_torch(imagem, tx, ty)\n",
        "                        nome_arquivo_saida = os.path.join(caminho_temporario_especie, f\"{nome_img}_trans_{k+1}.jpg\")\n",
        "                        save_image_pil(transladada, nome_arquivo_saida)\n",
        "                        contagem_aumento += 1\n",
        "\n",
        "                    # TRANSLAÇÕES DA IMAGEM ESPELHADA\n",
        "                    for k, (tx, ty) in enumerate(translacoes):\n",
        "                        transladada_espelhada = translate_image_torch(espelhada, tx, ty)\n",
        "                        nome_arquivo_saida = os.path.join(caminho_temporario_especie, f\"{nome_img}_flip_trans_{k+1}.jpg\")\n",
        "                        save_image_pil(transladada_espelhada, nome_arquivo_saida)\n",
        "                        contagem_aumento += 1\n",
        "\n",
        "                    total_aumentadas_especie += contagem_aumento\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao processar {caminho_img}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"   Aumento de dados local para {especie} concluído.\")\n",
        "            print(f\"   Imagens originais: {total_imagens_especie}, Aumentadas: {total_aumentadas_especie}\")\n",
        "            print(f\"   Fator de aumento: {total_aumentadas_especie/total_imagens_especie if total_imagens_especie > 0 else 0:.1f}x\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "\n",
        "            # 5. VERIFICAR CONEXÃO E RECONECTAR SE NECESSÁRIO\n",
        "            print(f\"\\nVerificando conexão com Google Drive para {especie}...\")\n",
        "            drive_montado = os.path.exists('/content/drive')\n",
        "            while not drive_montado:\n",
        "                print(\"   Google Drive não está montado ou foi desconectado.\")\n",
        "                print(\"   Tentando reconectar...\")\n",
        "                try:\n",
        "                    drive.mount('/content/drive', force_remount=True)\n",
        "                    drive_montado = os.path.exists('/content/drive')\n",
        "                    if drive_montado:\n",
        "                        print(\"   Google Drive reconectado com sucesso.\")\n",
        "                        # Pode ser útil adicionar uma pequena pausa após reconectar\n",
        "                        time.sleep(5)\n",
        "                    else:\n",
        "                         print(\"   Falha na reconexão. Verifique as permissões ou tente manualmente.\")\n",
        "                         # Adicionar um prompt para o usuário ou sair se a reconexão falhar repetidamente\n",
        "                         input(\"Pressione Enter após reconectar o Drive manualmente ou Crtl+C para cancelar...\")\n",
        "                         drive_montado = os.path.exists('/content/drive') # Verificar novamente após o prompt\n",
        "                except Exception as e:\n",
        "                    print(f\"   Erro durante a tentativa de reconexão: {str(e)}\")\n",
        "                    input(\"   Pressione Enter para tentar novamente ou Crtl+C para cancelar...\")\n",
        "                    drive_montado = os.path.exists('/content/drive') # Verificar novamente após o prompt\n",
        "\n",
        "            print(\"   Google Drive está conectado.\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "\n",
        "            # 6. COPIAR PARA O DRIVE (APENAS ESTA ESPÉCIE)\n",
        "            print(f\"\\nCOPIANDO DADOS AUMENTADOS DE {especie} PARA GOOGLE DRIVE...\")\n",
        "            print(f\"   De: {caminho_temporario_especie}\")\n",
        "            print(f\"   Para: {diretorio_saida_especie_drive}\")\n",
        "            print(\"   Esta operação pode demorar um pouco...\")\n",
        "\n",
        "            # Limpar o diretório de destino da espécie no Drive antes de copiar\n",
        "            if os.path.exists(diretorio_saida_especie_drive):\n",
        "                print(f\"   Removendo diretório antigo da espécie no Drive: {diretorio_saida_especie_drive}\")\n",
        "                shutil.rmtree(diretorio_saida_especie_drive)\n",
        "                print(\"   Diretório antigo da espécie removido.\")\n",
        "\n",
        "            os.makedirs(os.path.dirname(diretorio_saida_especie_drive), exist_ok=True) # Garantir diretório pai existe\n",
        "            shutil.copytree(caminho_temporario_especie, diretorio_saida_especie_drive) # Copiar a pasta da espécie\n",
        "\n",
        "            print(f\"   CÓPIA DE {especie} PARA GOOGLE DRIVE CONCLUÍDA!\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "\n",
        "            # 7. VERIFICAR RESULTADO NO DRIVE (APENAS ESTA ESPÉCIE)\n",
        "            print(f\"\\nVerificando resultado final de {especie} no Drive...\")\n",
        "            if os.path.exists(diretorio_saida_especie_drive):\n",
        "                contagem_img_drive = len([f for f in os.listdir(diretorio_saida_especie_drive)\n",
        "                                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                print(f\"   {especie} no Drive: {contagem_img_drive} imagens\")\n",
        "\n",
        "                if contagem_img_drive == total_aumentadas_especie:\n",
        "                    print(f\"   INTEGRIDADE VERIFICADA para {especie}: Todos os arquivos foram copiados corretamente!\")\n",
        "                else:\n",
        "                    print(f\"   AVISO: Possível perda de dados durante a cópia de {especie}!\")\n",
        "                    print(f\"      Local: {total_aumentadas_especie} imagens, Drive: {contagem_img_drive} imagens\")\n",
        "            else:\n",
        "                print(f\"   ERRO: O diretório de destino no Drive para {especie} não foi encontrado após a cópia!\")\n",
        "\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            # 8. LIMPAR ARQUIVOS TEMPORÁRIOS LOCAIS PARA ESTA ESPÉCIE\n",
        "            print(f\"\\nLimpando arquivos temporários locais para {especie}...\")\n",
        "            if os.path.exists(caminho_temporario_especie):\n",
        "                shutil.rmtree(caminho_temporario_especie)\n",
        "                print(f\"   Arquivos temporários de {especie} removidos.\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nERRO DURANTE O PROCESSAMENTO DE {especie}:\")\n",
        "            print(f\"   {str(e)}\")\n",
        "            print(f\"\\nPulando para a próxima espécie após tentativa de limpeza de emergência...\")\n",
        "\n",
        "            # Limpeza de emergência para a espécie atual\n",
        "            try:\n",
        "                if os.path.exists(caminho_temporario_especie):\n",
        "                    shutil.rmtree(caminho_temporario_especie)\n",
        "                    print(f\"   Arquivos temporários de {especie} removidos.\")\n",
        "            except:\n",
        "                print(f\"   Não foi possível remover arquivos temporários de {especie}\")\n",
        "\n",
        "            # Continue para a próxima espécie\n",
        "            continue\n",
        "\n",
        "    # 9. RESUMO FINAL (Após processar todas as espécies)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FLUXO GERAL CONCLUÍDO!\")\n",
        "    print(f\"Dataset aumentado disponível em: {caminho_saida}\")\n",
        "\n",
        "    # Contar total final no Drive\n",
        "    total_imagens_final_drive = 0\n",
        "    especies_final_drive = [d for d in os.listdir(caminho_saida)\n",
        "                           if os.path.isdir(os.path.join(caminho_saida, d))]\n",
        "\n",
        "    print(f\"\\nResumo final no Drive ({caminho_saida}):\")\n",
        "    for especie in especies_final_drive:\n",
        "        caminho_especie = os.path.join(caminho_saida, especie)\n",
        "        contagem_img = len([f for f in os.listdir(caminho_especie)\n",
        "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        total_imagens_final_drive += contagem_img\n",
        "        print(f\"   ├── {especie}: {contagem_img} imagens\")\n",
        "\n",
        "    print(f\"\\nTotal final de imagens no Drive: {total_imagens_final_drive}\")\n",
        "    print(f\"Total de espécies no Drive: {len(especies_final_drive)}\")\n",
        "\n",
        "    # Verificar se o número de espécies no Drive corresponde ao original\n",
        "    if len(especies_final_drive) == len(diretorios_especies):\n",
        "         print(\"   VERIFICADO: Número de espécies no Drive corresponde ao original.\")\n",
        "    else:\n",
        "         print(\"   AVISO: O número de espécies no Drive não corresponde ao original!\")\n",
        "\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "# Função auxiliar para verificar espaço em disco (mantida)\n",
        "def verificar_espaco_disco():\n",
        "    \"\"\"Verifica o espaço disponível no disco\"\"\"\n",
        "    total, usado, livre = shutil.disk_usage('/')\n",
        "\n",
        "    print(f\"Espaço em disco:\")\n",
        "    print(f\"Total: {total // (2**30):.1f} GB\")\n",
        "    print(f\"Usado: {usado // (2**30):.1f} GB\")\n",
        "    print(f\"Livre: {livre // (2**30):.1f} GB\")\n",
        "\n",
        "    if livre < 5 * (2**30):  # Aumentado para 5GB para safety\n",
        "        print(\"AVISO: Pouco espaço livre. O augmentation pode falhar.\")\n",
        "    else:\n",
        "        print(\"Espaço suficiente para data augmentation local.\")\n",
        "\n",
        "# --- Execução do Workflow ---\n",
        "\n",
        "# Verificar espaço primeiro\n",
        "verificar_espaco_disco()\n",
        "\n",
        "# Executar o workflow robusto, se as variáveis estiverem definidas\n",
        "if 'dataset_path' in globals() and 'output_path' in globals():\n",
        "    # Verificar se o diretório de entrada original realmente existe\n",
        "    if os.path.exists(dataset_path):\n",
        "        print(f\"\\nUSANDO PROCESSAMENTO LOCAL (POR ESPÉCIE) + CÓPIA FINAL PARA DRIVE\")\n",
        "        print(f\"   Isso resolve problemas de sincronização com Google Drive\")\n",
        "        print(f\"   Velocidade ~10x mais rápida\")\n",
        "        print(f\"   Maior resiliência a quedas de conexão do Drive\")\n",
        "\n",
        "        fluxo_aumento_dados_robusto(dataset_path, output_path)\n",
        "    else:\n",
        "        print(\"ERRO: dataset_path não existe!\")\n",
        "        print(f\"Caminho verificado: {dataset_path}\")\n",
        "else:\n",
        "    print(\"ERRO: Variáveis dataset_path e/ou output_path não estão definidas!\")\n",
        "    print(\"Execute primeiro as células de configuração do Google Drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTrEwcHjCRl1"
      },
      "source": [
        "## **Treinamento do modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_Hicqamyagk"
      },
      "source": [
        "Começamos especificando qual framework de Deep Learning o Keras usará, neste caso o PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wl5OdpOtuTaV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K35EBTZtzuP_"
      },
      "source": [
        "Após isto importamos o keras e pytorch para a parte das Redes Neurais e numpy, matplotlib, panda e scikit-learn para visualizações de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-IZcxp-1waqR"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV8d-XGpz1EX"
      },
      "source": [
        "Agora, fazemos um debug das importações e tomada de decisão: utilizar GPU caso tenha, caso contrário, utilizar a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p5AFlvkHEu9i",
        "outputId": "98c28e20-0d03-48e4-c435-7691660c650c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Versão do Keras: 3.11.3\n",
            "Keras está usando o backend: torch\n",
            "Usando o dispositivo: cuda\n"
          ]
        }
      ],
      "source": [
        "print(f\"Versão do Keras: {keras.__version__}\") # Printa a versão do Keras\n",
        "\n",
        "print(f\"Keras está usando o backend: {keras.backend.backend()}\") # Printa o framework utilizado pelo Keras\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Verifica se tem GPU para utilizar\n",
        "\n",
        "print(f\"Usando o dispositivo: {device}\") # Mostra a GPU/CPU que será utilizada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXSGoW4IaI5a"
      },
      "source": [
        "Agora, importamos a arquitetura do modelo EfficientNetV2 com a rede neural construida camada por camada e o modificamos para utilizarmos apenas a effnetv2_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SUe6OUikaB39"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creates a EfficientNetV2 Model as defined in:\n",
        "Mingxing Tan, Quoc V. Le. (2021).\n",
        "EfficientNetV2: Smaller Models and Faster Training\n",
        "arXiv preprint arXiv:2104.00298.\n",
        "import from https://github.com/d-li14/mobilenetv2.pytorch\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "__all__ = ['effnetv2_l']\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "# SiLU (Swish) activation function\n",
        "if hasattr(nn, 'SiLU'):\n",
        "    SiLU = nn.SiLU\n",
        "else:\n",
        "    # For compatibility with old PyTorch versions\n",
        "    class SiLU(nn.Module):\n",
        "        def forward(self, x):\n",
        "            return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, inp, oup, reduction=4):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
        "                SiLU(),\n",
        "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "def conv_3x3_bn(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
        "        super(MBConv, self).__init__()\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = round(inp * expand_ratio)\n",
        "        self.identity = stride == 1 and inp == oup\n",
        "        if use_se:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                SELayer(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # fused\n",
        "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.identity:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class EffNetV2(nn.Module):\n",
        "    def __init__(self, cfgs, num_classes=1000, width_mult=1.):\n",
        "        super(EffNetV2, self).__init__()\n",
        "        self.cfgs = cfgs\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(24 * width_mult, 8)\n",
        "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
        "        # building inverted residual blocks\n",
        "        block = MBConv\n",
        "        for t, c, n, s, use_se in self.cfgs:\n",
        "            output_channel = _make_divisible(c * width_mult, 8)\n",
        "            for i in range(n):\n",
        "                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t, use_se))\n",
        "                input_channel = output_channel\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        # building last several layers\n",
        "        output_channel = _make_divisible(1792 * width_mult, 8) if width_mult > 1.0 else 1792\n",
        "        self.conv = conv_1x1_bn(input_channel, output_channel)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Linear(output_channel, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.001)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def effnetv2_l(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-L model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  32,  4, 1, 0],\n",
        "        [4,  64,  7, 2, 0],\n",
        "        [4,  96,  7, 2, 0],\n",
        "        [4, 192, 10, 2, 1],\n",
        "        [6, 224, 19, 1, 1],\n",
        "        [6, 384, 25, 2, 1],\n",
        "        [6, 640,  7, 1, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}